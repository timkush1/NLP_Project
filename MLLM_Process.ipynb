{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install necessary libraries\n!pip install transformers datasets\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-25T14:07:39.853038Z","iopub.execute_input":"2024-10-25T14:07:39.853369Z","iopub.status.idle":"2024-10-25T14:08:12.706845Z","shell.execute_reply.started":"2024-10-25T14:07:39.853312Z","shell.execute_reply":"2024-10-25T14:08:12.705823Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Importing necessary libraries\nfrom transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\nfrom datasets import load_dataset\nimport pandas as pd\nimport random","metadata":{"execution":{"iopub.status.busy":"2024-10-25T14:08:31.151389Z","iopub.execute_input":"2024-10-25T14:08:31.151691Z","iopub.status.idle":"2024-10-25T14:08:31.159407Z","shell.execute_reply.started":"2024-10-25T14:08:31.151658Z","shell.execute_reply":"2024-10-25T14:08:31.158663Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Step 3: Load the Dataset\nlanguages = ['en', 'es', 'fr', 'hi', 'sw', 'zh']  # High-resource and low-resource languages\ndatasets = {}\n\n# Load the XNLI dataset for each language and store it in a dictionary\nfor lang in languages:\n    datasets[lang] = load_dataset(\"xnli\", lang)","metadata":{"execution":{"iopub.status.busy":"2024-10-25T14:11:31.369023Z","iopub.execute_input":"2024-10-25T14:11:31.369743Z","iopub.status.idle":"2024-10-25T14:12:03.941363Z","shell.execute_reply.started":"2024-10-25T14:11:31.369698Z","shell.execute_reply":"2024-10-25T14:12:03.940385Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/20.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44254094c39a473b995f3c45d12556dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/50.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"110d2591f5e74eb6aa508d83e995d4ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/308k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"711711d8d037433ba45bd9d0e4f03ba0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/157k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d8ec0c17a514c308d689ea33175a04e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/392702 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24a6b60b0a3645f8a3885140608ce458"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/5010 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a10185f972734ab4aa3c5ab504f46fbf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/2490 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90a61581ec2f4efaa865fa58354c33e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/53.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c4a7f349ca24d56a060fbe4191d1a47"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/342k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa366276393c4e389232ea3b7a878bc0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/173k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1906de3dcbeb457cac76ad8d1adaf009"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/392702 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f268bfe3f65942b1b752a73f3e618cb7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/5010 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c85406d5dabb4c54a0c589ad7fd49edc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/2490 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1e64f7117eb4e97b442529335c50321"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/55.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64d9991a0ae444f7984d8626b7fdfee7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/360k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cedbb3edb514c6b89f5528b561c259f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/183k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15c3989eb84448999cbb5748905940e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/392702 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b9c1ca1c72b412dbf319b66e75f9c11"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/5010 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4bc80272416e4ee0869d615f9453209d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/2490 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc83faf73c96441fb1379cc6dbbf946c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/70.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecb694caab364894baab125c5710738b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb8a9d09c4e04c1ab051e1e2d35cd636"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/249k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24651b10fca148aeafba9bb77fb696d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/392702 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b1d76ed30044f25ac0456ac99bbca85"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/5010 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7614162da794ce39d98b734cfdd8d2e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/2490 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4c5a3ea88be4c3b853c42549d0b5c75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/45.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"643513402e654c419da2100948f1ce35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/312k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4fe762bd06a42dfa5ad850a7edef7d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/158k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f15fc05e3e34e9098013801ef68fce5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/392702 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a22df553d5b448b8004b3f597812ef7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/5010 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"986bf057114348e4a8221fee241a2376"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/2490 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9a1b53f6df4440aa765191b232d49a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/47.8M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56bca2a9711c4271a3f7d7eda96817dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/310k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d97ebdcd4f849c3b0c946c8e4009f23"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/157k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc2d3466c392449296ba972266246464"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/392702 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5546bf962794b71bfe8137618e3ef62"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/5010 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4b0c5fe9d6e477cab208ddf207a4c29"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/2490 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be49f44eaf27452bbb5d85ab3456887b"}},"metadata":{}}]},{"cell_type":"code","source":"# Convert the dataset to a Pandas DataFrame for easier manipulation\n# Use select to take 100 samples from each language for simplicity\nall_data = []\nfor lang in languages:\n    dataset_samples = datasets[lang]['train'].select(range(100))\n    for sample in dataset_samples:\n        all_data.append(sample)\n\n# Create a DataFrame from the collected data\ndata = pd.DataFrame(all_data)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-25T14:18:19.484469Z","iopub.execute_input":"2024-10-25T14:18:19.485477Z","iopub.status.idle":"2024-10-25T14:18:19.533550Z","shell.execute_reply.started":"2024-10-25T14:18:19.485432Z","shell.execute_reply":"2024-10-25T14:18:19.532588Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Step 4: Prepare Data for the Model\n# Create a dictionary to hold queries (premise + hypothesis pairs) and their corresponding expected answers for each language. This dictionary will be used for model evaluation.\n\n# Creating a dictionary for each language's queries and answers\nqueries_answers = {}\nfor lang in languages:\n    # Filter the data for each language using the premise and hypothesis columns\n    lang_data = data[data['premise'].str.contains(lang, na=False)]  # Adjust this if you have a better condition for language detection\n    queries_answers[lang] = list( lang_data['hypothesis'] ,zip(lang_data['premise'] , lang_data['label']))\n\n# Continue with Step 5 and beyond as usual\n","metadata":{"execution":{"iopub.status.busy":"2024-10-25T14:28:03.388018Z","iopub.execute_input":"2024-10-25T14:28:03.388977Z","iopub.status.idle":"2024-10-25T14:28:03.401728Z","shell.execute_reply.started":"2024-10-25T14:28:03.388933Z","shell.execute_reply":"2024-10-25T14:28:03.400789Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2024-10-25T14:24:05.877968Z","iopub.execute_input":"2024-10-25T14:24:05.878767Z","iopub.status.idle":"2024-10-25T14:24:05.899212Z","shell.execute_reply.started":"2024-10-25T14:24:05.878721Z","shell.execute_reply":"2024-10-25T14:24:05.898248Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"                                               premise  \\\n0    Conceptually cream skimming has two basic dime...   \n1    you know during the season and i guess at at y...   \n2    One of our number will carry out your instruct...   \n3    How do you know ? All this is their informatio...   \n4    yeah i tell you what though if you go price so...   \n..                                                 ...   \n595                   点击 更 多 链接 ( 在 杂项 下 的 右手 边 ) , 以及   \n596        所以 我 开始 看 它 , 所有 的 突然 留下 关注 下周 , 我 去 了 什么 ,   \n597                               哦 , 不 , 哦 , 好 吧 , 保重   \n598                                        \" 你好 , 本.''   \n599                                          你 怎么 能 证明   \n\n                                            hypothesis  label  \n0    Product and geography are what make cream skim...      1  \n1    You lose the things to the following level if ...      0  \n2    A member of my team will execute your orders w...      0  \n3                   This information belongs to them .      0  \n4            The tennis shoes have a range of prices .      1  \n..                                                 ...    ...  \n595                                      没有 链接 点击 杂项 .      2  \n596                         如果 我 知道 , 我 就 不 会 开始 看 它 了      1  \n597                                              再 见 了      0  \n598                                           我 忽略 了 本      2  \n599                                 你 能 告诉 我 怎么 证明 吗 ?      0  \n\n[600 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>premise</th>\n      <th>hypothesis</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Conceptually cream skimming has two basic dime...</td>\n      <td>Product and geography are what make cream skim...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>you know during the season and i guess at at y...</td>\n      <td>You lose the things to the following level if ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>One of our number will carry out your instruct...</td>\n      <td>A member of my team will execute your orders w...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>How do you know ? All this is their informatio...</td>\n      <td>This information belongs to them .</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>yeah i tell you what though if you go price so...</td>\n      <td>The tennis shoes have a range of prices .</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>595</th>\n      <td>点击 更 多 链接 ( 在 杂项 下 的 右手 边 ) , 以及</td>\n      <td>没有 链接 点击 杂项 .</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>596</th>\n      <td>所以 我 开始 看 它 , 所有 的 突然 留下 关注 下周 , 我 去 了 什么 ,</td>\n      <td>如果 我 知道 , 我 就 不 会 开始 看 它 了</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>597</th>\n      <td>哦 , 不 , 哦 , 好 吧 , 保重</td>\n      <td>再 见 了</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>598</th>\n      <td>\" 你好 , 本.''</td>\n      <td>我 忽略 了 本</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>599</th>\n      <td>你 怎么 能 证明</td>\n      <td>你 能 告诉 我 怎么 证明 吗 ?</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>600 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Step 5: Set Up Text Generation Pipeline\n# Set up the LLaMA model and its tokenizer. We use a smaller version like \"open_llama_3b\" for computational efficiency.\n\n# Load LLaMA model\nmodel_name = \"openlm-research/open_llama_3b\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n# model = AutoModelForCausalLM.from_pretrained(model_name)\n\n# Check if pad_token_id is missing, and set it to eos_token_id if needed\nif tokenizer.pad_token_id is None:\n    tokenizer.pad_token_id = tokenizer.eos_token_id\n\n# Set up the text generation pipeline using LLaMA\ngenerator = pipeline(\n    \"text-generation\",\n    model=model_name,\n    tokenizer=tokenizer,\n    return_full_text=False,\n    torch_dtype=\"auto\",  # Automatically select the best precision\n    trust_remote_code=True,\n    device_map=\"auto\",\n    temperature=0.2,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n    no_repeat_ngram_size=3,\n    max_new_tokens=150,\n    do_sample=False,\n    num_return_sequences=1,\n    eos_token_id=tokenizer.eos_token_id,\n    repetition_penalty=1.1,  # Prevent output looping\n    output_attentions=False,\n)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-25T14:32:09.352036Z","iopub.execute_input":"2024-10-25T14:32:09.352781Z","iopub.status.idle":"2024-10-25T14:32:41.049422Z","shell.execute_reply.started":"2024-10-25T14:32:09.352736Z","shell.execute_reply":"2024-10-25T14:32:41.048512Z"},"trusted":true},"execution_count":29,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/593 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9fb057bbab384f12b0eb14db9a10820f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/534k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b795dfc581a4e92b29920ee5dcb56d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/330 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"014671ab36c94c0dadd737e4fb72e2a4"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\nYou are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/506 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c48cc5d92c8c40f2b9e48623d457f389"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/6.85G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0bece8c12714955b0f1f2350fb31e14"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b06ce3ec73d457487118a9815b02878"}},"metadata":{}}]},{"cell_type":"code","source":"### Step 6: Define Evaluation Function for Model\n# We will now continue with evaluating the model using the corrected `queries_answers` dictionary.\n\ndef evaluate_model_on_queries(queries_answers, num_samples=10):\n    # This function will evaluate the model on a sample of queries for each language.\n    results = []\n\n    for lang, queries in queries_answers.items():\n        print(f\"Evaluating language: {lang}\")\n\n        # Randomly sample a few queries from each language\n        sample_queries = random.sample(queries, min(num_samples, len(queries)))\n\n        for premise, hypothesis, expected_label in sample_queries:\n            # Concatenate premise and hypothesis to create the input query\n            query = f\"Premise: {premise} Hypothesis: {hypothesis} Is the hypothesis true given the premise you have 3 options to answer?\"\n\n            # Generate a response from the model\n            response = generator(query, max_new_tokens=50, num_return_sequences=1, truncation=True)[0]['generated_text']\n\n            # Log the results including the original query, expected answer, and the response generated by the model\n            results.append({\n                \"language\": lang,\n                \"premise\": premise,\n                \"hypothesis\": hypothesis,\n                \"expected_label\": expected_label,\n                \"response\": response\n            })\n\n    # Convert results to a DataFrame for easy inspection\n    return pd.DataFrame(results)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-25T14:37:27.982923Z","iopub.execute_input":"2024-10-25T14:37:27.983893Z","iopub.status.idle":"2024-10-25T14:37:27.991172Z","shell.execute_reply.started":"2024-10-25T14:37:27.983847Z","shell.execute_reply":"2024-10-25T14:37:27.990233Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# Step 7: Perform Model Evaluation\n# Evaluate the model on the sample queries from the dataset\nevaluation_results = evaluate_model_on_queries(queries_answers)","metadata":{"execution":{"iopub.status.busy":"2024-10-25T14:37:35.690832Z","iopub.execute_input":"2024-10-25T14:37:35.691860Z","iopub.status.idle":"2024-10-25T14:38:49.894472Z","shell.execute_reply.started":"2024-10-25T14:37:35.691801Z","shell.execute_reply":"2024-10-25T14:38:49.893471Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"Evaluating language: en\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\nStarting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\nYou seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","output_type":"stream"},{"name":"stdout","text":"Evaluating language: es\nEvaluating language: fr\nEvaluating language: hi\nEvaluating language: sw\nEvaluating language: zh\n","output_type":"stream"}]},{"cell_type":"code","source":"# Step 8: Save and Display Evaluation Results\n# Save evaluation results to a CSV file and display the first few rows\nevaluation_results.to_csv(\"multilingual_evaluation_results.csv\", index=False)\nprint(evaluation_results.head())","metadata":{"execution":{"iopub.status.busy":"2024-10-25T14:39:05.496886Z","iopub.execute_input":"2024-10-25T14:39:05.497860Z","iopub.status.idle":"2024-10-25T14:39:05.510975Z","shell.execute_reply.started":"2024-10-25T14:39:05.497813Z","shell.execute_reply":"2024-10-25T14:39:05.510059Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"  language                                            premise  \\\n0       en  Kuweka hatua kando kama kufikisha ya farasi , ...   \n1       en  The poverty , for instance , does not create t...   \n2       en                      Bonjour , ben . ' ' ' ' ' ' '   \n3       en  Yeah i tell you what though ukienda bei baadhi...   \n4       en  and so i started watching it and all of a sudd...   \n\n                                          hypothesis  expected_label  \\\n0  Gail sheehy ni shabaha maarufu kwa masimango k...               1   \n1  Poverty doesn 't create a sense of shame in an...               2   \n2                                   J' ai ignoré ben               2   \n3              Viatu vya tennis zina masafa ya bei .               1   \n4  I wouldn 't have started watching it if I 'd k...               1   \n\n                                            response  \n0  \\nThe purpose of this study is to find out whe...  \n1  \\nPremise: Poverty is a major cause of crime i...  \n2  \\nJ' ais ignoré Ben is the hypothesis.\\nJe sui...  \n3  \\nPremise: The premise is that the value of a ...  \n4  \\nI'm not sure what you mean by \"started watch...  \n","output_type":"stream"}]},{"cell_type":"code","source":"# Step 9: Error Analysis\n# Perform basic error analysis to identify discrepancies between expected and generated answers.\ndef perform_error_analysis(evaluation_results):\n    # Extract responses that do not match the expected label\n    # Assuming we categorize the response manually into labels similar to expected labels (0, 1, 2)\n    incorrect_results = evaluation_results[evaluation_results['response'] != evaluation_results['expected_label']]\n    return incorrect_results\n\n# Perform error analysis\nerrors = perform_error_analysis(evaluation_results)\n\n# Save errors to CSV\nerrors.to_csv(\"error_analysis_results.csv\", index=False)\n\n# Display the first few rows of the error analysis results\nprint(errors.head())","metadata":{"execution":{"iopub.status.busy":"2024-10-25T14:39:09.765946Z","iopub.execute_input":"2024-10-25T14:39:09.766629Z","iopub.status.idle":"2024-10-25T14:39:09.779332Z","shell.execute_reply.started":"2024-10-25T14:39:09.766586Z","shell.execute_reply":"2024-10-25T14:39:09.778320Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"  language                                            premise  \\\n0       en  Kuweka hatua kando kama kufikisha ya farasi , ...   \n1       en  The poverty , for instance , does not create t...   \n2       en                      Bonjour , ben . ' ' ' ' ' ' '   \n3       en  Yeah i tell you what though ukienda bei baadhi...   \n4       en  and so i started watching it and all of a sudd...   \n\n                                          hypothesis  expected_label  \\\n0  Gail sheehy ni shabaha maarufu kwa masimango k...               1   \n1  Poverty doesn 't create a sense of shame in an...               2   \n2                                   J' ai ignoré ben               2   \n3              Viatu vya tennis zina masafa ya bei .               1   \n4  I wouldn 't have started watching it if I 'd k...               1   \n\n                                            response  \n0  \\nThe purpose of this study is to find out whe...  \n1  \\nPremise: Poverty is a major cause of crime i...  \n2  \\nJ' ais ignoré Ben is the hypothesis.\\nJe sui...  \n3  \\nPremise: The premise is that the value of a ...  \n4  \\nI'm not sure what you mean by \"started watch...  \n","output_type":"stream"}]},{"cell_type":"code","source":"prefix = \"you are an helpful assistant, i will give 2 sentences one is the Premise and the second one is Hypothesis. you need to decide about the relation between the Premise and Hypothesis you can choose in the output one of the three options: 1.condradiction , 2.Entailment 3. Neutral?\"\nquery = f\"Premise: i am happy  Hypothesis: i am sad Is the hypothesis true\"\nres = \"response only in one of the following answers:  1.condradiction , 2.Entailment 3. Neutral?\"\nconcat = prefix + \"\\n\" + res+ \"\\n\" + res+\"\\n\" + res+\"\\n\" + res+ \"\\n\" +  query  \n# Generate a response from the model\nresponse = generator(query, max_new_tokens=50, num_return_sequences=1, truncation=True,temperature = 0.1)[0]['generated_text']","metadata":{"execution":{"iopub.status.busy":"2024-10-25T14:53:16.190368Z","iopub.execute_input":"2024-10-25T14:53:16.190778Z","iopub.status.idle":"2024-10-25T14:53:17.645496Z","shell.execute_reply.started":"2024-10-25T14:53:16.190738Z","shell.execute_reply":"2024-10-25T14:53:17.644684Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"response","metadata":{"execution":{"iopub.status.busy":"2024-10-25T14:53:19.949027Z","iopub.execute_input":"2024-10-25T14:53:19.949465Z","iopub.status.idle":"2024-10-25T14:53:19.955645Z","shell.execute_reply.started":"2024-10-25T14:53:19.949423Z","shell.execute_reply":"2024-10-25T14:53:19.954635Z"},"trusted":true},"execution_count":59,"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"'?\\nPremise: I am happy Hypotheses: I will be happy if I get a new car. I will not be happy If I don’t get a car.\\nP1: I have a new house.\\nH'"},"metadata":{}}]},{"cell_type":"code","source":"# Define the prefix and a set of multiple examples to fine-tune the model for better understanding\nprefix = \"You are a helpful assistant. I will give 2 sentences: one is the Premise and the second one is the Hypothesis. You need to decide on the relation between the Premise and Hypothesis. You can choose one of the following answers: 1. Contradiction, 2. Entailment, 3. Neutral.\"\n\n# Adding multiple examples to help the model better understand the task\nexample_1 = \"Premise: The sky is blue. Hypothesis: The sky is not blue.\"\nexample_2 = \"Premise: John is in New York. Hypothesis: John is in the USA.\"\nexample_3 = \"Premise: Mary likes coffee. Hypothesis: Mary dislikes coffee.\"\nexample_4 = \"Premise: It is raining today. Hypothesis: It is sunny today.\"\nexample_5 = \"Premise: All dogs are mammals. Hypothesis: Some dogs are not mammals.\"\nexample_6 = \"Premise: The store opens at 9 AM. Hypothesis: The store is closed at 10 AM.\"\nexample_7 = \"Premise: Tom went to the gym. Hypothesis: Tom worked out today.\"\nexample_8 = \"Premise: Sarah finished her homework. Hypothesis: Sarah has completed her studies.\"\nexample_9 = \"Premise: The book is on the table. Hypothesis: The book is in the drawer.\"\nexample_10 = \"Premise: The car is red. Hypothesis: The vehicle is colored blue.\"\n\n# Specify the response expectations to ensure the model stays within the three options\nres = \"Respond only with one of the following: 1. Contradiction, 2. Entailment, 3. Neutral.\"\n\n# Concatenate the prefix, multiple examples, and the response instruction to create a complete prompt\nconcat = (\n    f\"{prefix}\\n{res}\\n\"\n    f\"Example 1: {example_1}\\n{res}\\n\"\n    f\"Example 2: {example_2}\\n{res}\\n\"\n    f\"Example 3: {example_3}\\n{res}\\n\"\n    f\"Example 4: {example_4}\\n{res}\\n\"\n    f\"Example 5: {example_5}\\n{res}\\n\"\n    f\"Example 6: {example_6}\\n{res}\\n\"\n    f\"Example 7: {example_7}\\n{res}\\n\"\n    f\"Example 8: {example_8}\\n{res}\\n\"\n    f\"Example 9: {example_9}\\n{res}\\n\"\n    f\"Example 10: {example_10}\\n{res}\\n\"\n)\n\n# Example of the premise and hypothesis to be evaluated\nquery = \"Premise: The sun rises in the east. Hypothesis: The sun rises in the west. Is the hypothesis true?\"\n\n# Concatenate the query with the previous context\nconcat += f\"\\n{query}\"\n\n# Generate a response from the model using the prompt\nresponse = generator(concat, max_new_tokens=50, num_return_sequences=1, truncation=True, temperature=0.1)[0]['generated_text']\n\n# Display the response\nprint(response)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-25T15:00:41.624250Z","iopub.execute_input":"2024-10-25T15:00:41.624737Z","iopub.status.idle":"2024-10-25T15:00:43.305313Z","shell.execute_reply.started":"2024-10-25T15:00:41.624695Z","shell.execute_reply":"2024-10-25T15:00:43.304274Z"},"trusted":true},"execution_count":60,"outputs":[{"name":"stdout","text":"\n\nA: The answer is \"Contradiction\".\nThe premise is true, but the hypothesis is false.\n\n","output_type":"stream"}]}]}